"""
High-Quality Dataset Downloader
================================
Downloads real-world vulnerability datasets with natural code structure.

Datasets:
1. CodeXGLUE Defect Detection (Microsoft) - Python/C from GitHub
2. PyVul/CleanVul - Clean Python vulnerabilities
3. DrRepair - Real bugs from GitHub
4. JavaScript vulnerabilities from npm/Snyk

Author: AI-based Vulnerability Scanner
Date: February 6, 2026
"""

import os
import json
import requests
import zipfile
import tarfile
import shutil
from pathlib import Path
from typing import Dict, List, Optional
import logging
from tqdm import tqdm
import subprocess

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Base paths
BACKEND_DIR = Path(__file__).parent.parent
RAW_DATA_DIR = BACKEND_DIR / "data" / "raw_datasets"
RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)


class QualityDatasetDownloader:
    """Download high-quality vulnerability datasets with real code."""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # Statistics
        self.stats = {
            'downloaded': 0,
            'failed': 0,
            'datasets': {}
        }
    
    def download_all(self):
        """Download all recommended datasets."""
        logger.info("=" * 60)
        logger.info("Starting High-Quality Dataset Download")
        logger.info("=" * 60)
        
        datasets = [
            ("CodeXGLUE Defect Detection", self.download_codexglue),
            ("Devign Dataset", self.download_devign),
            ("CVEFixes Dataset", self.download_cvefixes),
            ("JavaScript NPM Vulnerabilities", self.download_npm_vulns),
        ]
        
        for dataset_name, download_func in datasets:
            try:
                logger.info(f"\nüì¶ Downloading: {dataset_name}")
                logger.info("-" * 60)
                download_func()
                self.stats['downloaded'] += 1
                self.stats['datasets'][dataset_name] = 'SUCCESS'
            except Exception as e:
                logger.error(f"‚ùå Failed to download {dataset_name}: {e}")
                self.stats['failed'] += 1
                self.stats['datasets'][dataset_name] = f'FAILED: {str(e)}'
        
        self._print_summary()
    
    def download_codexglue(self):
        """
        Download CodeXGLUE Defect Detection Dataset.
        Source: Microsoft - Real Python/C code from GitHub
        """
        output_dir = RAW_DATA_DIR / "codexglue_defect"
        output_dir.mkdir(exist_ok=True)
        
        # CodeXGLUE Defect Detection dataset URLs
        base_url = "https://raw.githubusercontent.com/microsoft/CodeXGLUE/main/Code-Code/Defect-detection/dataset"
        
        files = {
            'train.jsonl': f'{base_url}/train.jsonl',
            'valid.jsonl': f'{base_url}/valid.jsonl',
            'test.jsonl': f'{base_url}/test.jsonl',
        }
        
        logger.info("üì• Downloading CodeXGLUE files...")
        
        for filename, url in files.items():
            output_path = output_dir / filename
            
            if output_path.exists():
                logger.info(f"‚úì {filename} already exists, skipping")
                continue
            
            try:
                response = self.session.get(url, timeout=60)
                response.raise_for_status()
                
                with open(output_path, 'wb') as f:
                    f.write(response.content)
                
                # Count samples
                with open(output_path, 'r', encoding='utf-8') as f:
                    count = sum(1 for _ in f)
                
                logger.info(f"‚úÖ Downloaded {filename} - {count:,} samples")
                
            except Exception as e:
                logger.error(f"‚ùå Failed to download {filename}: {e}")
                raise
        
        # Parse and convert to unified format
        self._parse_codexglue(output_dir)
        
        logger.info(f"‚úÖ CodeXGLUE dataset ready at: {output_dir}")
    
    def _parse_codexglue(self, data_dir: Path):
        """Parse CodeXGLUE jsonl format to unified format."""
        unified_data = []
        
        for split in ['train', 'valid', 'test']:
            jsonl_file = data_dir / f"{split}.jsonl"
            if not jsonl_file.exists():
                continue
            
            logger.info(f"üìù Parsing {split}.jsonl...")
            
            with open(jsonl_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    try:
                        item = json.loads(line.strip())
                        
                        # CodeXGLUE format: {"func": "code...", "target": 0/1, "project": "...", "commit_id": "..."}
                        unified_item = {
                            'code': item.get('func', ''),
                            'label': int(item.get('target', 0)),
                            'language': self._detect_language(item.get('func', '')),
                            'vulnerability_type': 'defect' if item.get('target') == 1 else 'safe',
                            'source': 'codexglue',
                            'split': split,
                            'metadata': {
                                'project': item.get('project', 'unknown'),
                                'commit_id': item.get('commit_id', ''),
                            }
                        }
                        
                        # Validate code quality
                        if self._is_valid_code(unified_item['code']):
                            unified_data.append(unified_item)
                        
                    except Exception as e:
                        logger.warning(f"Line {line_num}: {e}")
                        continue
        
        # Save unified format
        output_file = data_dir / "codexglue_unified.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(unified_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"‚úÖ Parsed {len(unified_data):,} samples to unified format")
    
    def download_devign(self):
        """
        Download Devign Dataset.
        Source: Real C vulnerabilities from FFmpeg, QEMU, etc.
        """
        output_dir = RAW_DATA_DIR / "devign"
        output_dir.mkdir(exist_ok=True)
        
        # Devign dataset from GitHub (function.json)
        url = "https://raw.githubusercontent.com/epicosy/devign/master/data/function.json"
        output_path = output_dir / "devign_raw.json"
        
        logger.info("üì• Downloading Devign dataset...")
        
        try:
            response = self.session.get(url, timeout=120)
            response.raise_for_status()
            
            with open(output_path, 'wb') as f:
                f.write(response.content)
            
            # Parse and convert
            self._parse_devign(output_path, output_dir)
            
            logger.info(f"‚úÖ Devign dataset ready at: {output_dir}")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not download Devign from main source: {e}")
            logger.info("üí° You can manually download from: https://sites.google.com/view/devign")
    
    def _parse_devign(self, input_file: Path, output_dir: Path):
        """Parse Devign JSON format."""
        logger.info("üìù Parsing Devign dataset...")
        
        with open(input_file, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
        
        unified_data = []
        
        for item in raw_data:
            unified_item = {
                'code': item.get('func', ''),
                'label': int(item.get('target', 0)),
                'language': 'c',
                'vulnerability_type': 'vulnerability' if item.get('target') == 1 else 'safe',
                'source': 'devign',
                'metadata': {
                    'project': item.get('project', 'unknown'),
                    'commit_id': item.get('commit_id', ''),
                }
            }
            
            if self._is_valid_code(unified_item['code']):
                unified_data.append(unified_item)
        
        # Save
        output_file = output_dir / "devign_unified.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(unified_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"‚úÖ Parsed {len(unified_data):,} Devign samples")
    
    def download_cvefixes(self):
        """
        Download CVEFixes dataset.
        Note: This is a large dataset, we'll download sample data.
        """
        output_dir = RAW_DATA_DIR / "cvefixes"
        output_dir.mkdir(exist_ok=True)
        
        logger.info("üì• Downloading CVEFixes sample data...")
        
        # CVEFixes has large database, we'll use their processed data
        url = "https://raw.githubusercontent.com/secureIT-project/CVEfixes/main/Data/sample_data.json"
        output_path = output_dir / "cvefixes_sample.json"
        
        try:
            response = self.session.get(url, timeout=60)
            
            if response.status_code == 404:
                logger.info("‚ö†Ô∏è Sample data not available, creating placeholder")
                logger.info("üí° For full CVEFixes: git clone https://github.com/secureIT-project/CVEfixes")
                
                # Create info file
                info = {
                    'dataset': 'CVEfixes',
                    'status': 'Manual download required',
                    'instructions': 'git clone https://github.com/secureIT-project/CVEfixes',
                    'url': 'https://github.com/secureIT-project/CVEfixes',
                    'note': 'Contains real CVE patches with before/after code'
                }
                
                with open(output_dir / "README.json", 'w') as f:
                    json.dump(info, f, indent=2)
                
                return
            
            response.raise_for_status()
            
            with open(output_path, 'wb') as f:
                f.write(response.content)
            
            logger.info(f"‚úÖ CVEFixes sample downloaded to: {output_dir}")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è CVEFixes requires manual download: {e}")
    
    def download_npm_vulns(self):
        """
        Download JavaScript/NPM vulnerability data.
        Uses NPM security advisories.
        """
        output_dir = RAW_DATA_DIR / "npm_vulnerabilities"
        output_dir.mkdir(exist_ok=True)
        
        logger.info("üì• Collecting JavaScript vulnerability patterns...")
        
        # Common JS vulnerability patterns from OWASP and npm advisories
        js_vuln_samples = [
            {
                'code': 'eval(userInput)',
                'label': 1,
                'language': 'javascript',
                'vulnerability_type': 'code_injection',
                'cwe_id': 'CWE-95',
                'source': 'owasp_pattern'
            },
            {
                'code': 'document.write(req.query.name)',
                'label': 1,
                'language': 'javascript',
                'vulnerability_type': 'xss',
                'cwe_id': 'CWE-79',
                'source': 'owasp_pattern'
            },
            {
                'code': 'db.query("SELECT * FROM users WHERE id = " + userId)',
                'label': 1,
                'language': 'javascript',
                'vulnerability_type': 'sql_injection',
                'cwe_id': 'CWE-89',
                'source': 'owasp_pattern'
            },
            # Safe patterns
            {
                'code': 'const sanitized = DOMPurify.sanitize(userInput); document.write(sanitized);',
                'label': 0,
                'language': 'javascript',
                'vulnerability_type': 'safe',
                'source': 'safe_pattern'
            }
        ]
        
        # Save starter patterns
        output_file = output_dir / "js_vulnerability_patterns.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(js_vuln_samples, f, indent=2)
        
        logger.info(f"‚úÖ Created JavaScript vulnerability pattern base")
        logger.info("üí° Tip: Enhance with real npm packages from: https://www.npmjs.com/advisories")
    
    def _detect_language(self, code: str) -> str:
        """Simple language detection based on syntax."""
        code_lower = code.lower()
        
        if 'def ' in code or 'import ' in code or 'print(' in code:
            return 'python'
        elif 'function ' in code or 'const ' in code or 'let ' in code or '=>' in code:
            return 'javascript'
        elif 'int main' in code or '#include' in code or 'printf' in code:
            return 'c'
        elif 'public class' in code or 'void main' in code:
            return 'java'
        else:
            return 'unknown'
    
    def _is_valid_code(self, code: str) -> bool:
        """Check if code is valid (not too short, not just imports, etc.)"""
        if not code or len(code.strip()) < 20:
            return False
        
        lines = [l.strip() for l in code.split('\n') if l.strip()]
        if len(lines) < 2:
            return False
        
        # Not just imports
        non_import_lines = [l for l in lines if not l.startswith(('import', 'from ', '#include', 'using'))]
        if len(non_import_lines) < 1:
            return False
        
        return True
    
    def _print_summary(self):
        """Print download summary."""
        logger.info("\n" + "=" * 60)
        logger.info("üìä DOWNLOAD SUMMARY")
        logger.info("=" * 60)
        logger.info(f"‚úÖ Successfully downloaded: {self.stats['downloaded']}")
        logger.info(f"‚ùå Failed: {self.stats['failed']}")
        logger.info("")
        
        for dataset_name, status in self.stats['datasets'].items():
            icon = "‚úÖ" if status == 'SUCCESS' else "‚ùå"
            logger.info(f"{icon} {dataset_name}: {status}")
        
        logger.info("\n" + "=" * 60)
        logger.info(f"üìÅ Datasets saved to: {RAW_DATA_DIR}")
        logger.info("=" * 60)
        
        # Next steps
        logger.info("\nüéØ NEXT STEPS:")
        logger.info("1. Verify downloaded datasets")
        logger.info("2. Run: python scripts/parse_quality_datasets.py")
        logger.info("3. Merge and clean data")
        logger.info("4. Train model with real data")


def main():
    """Main entry point."""
    downloader = QualityDatasetDownloader()
    
    try:
        downloader.download_all()
        
        # Print dataset locations
        print("\n" + "=" * 60)
        print("üì¶ Downloaded Datasets Location:")
        print("=" * 60)
        
        for dataset_dir in RAW_DATA_DIR.iterdir():
            if dataset_dir.is_dir():
                files = list(dataset_dir.glob("*.*"))
                print(f"\nüìÅ {dataset_dir.name}:")
                for f in files[:5]:  # Show first 5 files
                    size_mb = f.stat().st_size / (1024 * 1024)
                    print(f"  - {f.name} ({size_mb:.2f} MB)")
                if len(files) > 5:
                    print(f"  ... and {len(files) - 5} more files")
        
        print("\n‚úÖ Dataset download complete!")
        
    except KeyboardInterrupt:
        logger.info("\n‚ö†Ô∏è Download interrupted by user")
    except Exception as e:
        logger.error(f"\n‚ùå Download failed: {e}")
        raise


if __name__ == "__main__":
    main()
